{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe2009",
   "metadata": {},
   "source": [
    "# Offline preparations for L2 Sea Ice Concentration (SIC) and Sea Ice Edge (SIED) algorithms\n",
    "\n",
    "**Authors**: T. Lavergne (METNO)\n",
    "\n",
    "This notebook implements a series of \"offline\" preparation steps for the Level-2 SIC3H and SIED3H algorithms for the CIMR mission.\n",
    "\n",
    "Generally, *offline* preparations are steps that are run on a regular basis (e.g. every hours, every day, etc...). They aim at preparing everything needed for the L2 algorithms to run efficiently at once a new l2pp file is available. For example, external Auxiliary Data Files (ADFs) can be fetched and pre-processed on an hourly basis by an *offline* chain, to minimize the waiting time when a new l2pp product arrives.\n",
    "\n",
    "Specifically, this notebook implements the dynamic tuning of the SIC algorithm against a rolling archive of l2pp files, as is the case in the EUMETSAT OSI SAF and ESA CCI processing chains. In an operational, the rolling archive gives access to typically 5-10 days of L2PP data (full L2PP files or subsets of OZA-compensated TBs and auxiliary fields). In this demonstration prototype, we have to use the only simulated l2pp files we have, which is the SCEPS Polar Scene 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bd61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from glob import glob\n",
    "from importlib import reload\n",
    "\n",
    "# modules in the L2 Sea Ice Concentration ATBD (v2) contain software code that implement the SIC algorithm\n",
    "# 'local' software modules to implement the SIC algorithm\n",
    "_syspath = list(sys.path)\n",
    "for path in _syspath:\n",
    "    if '/algorithm/' in _syspath:\n",
    "        break\n",
    "    else:\n",
    "        sys.path.insert(0, os.path.abspath('../algorithm'))\n",
    "        break\n",
    "from sirrdp import rrdp_file\n",
    "from pmr_sic import tiepoints as tp\n",
    "from pmr_sic import hybrid_algo\n",
    "\n",
    "# modules to read CIMR l2pp files (for the dynamic tuning against an archive of past l2pp files)\n",
    "# 'l2ppl2 bridge' software modules to read the l2pp files\n",
    "_syspath = list(sys.path)\n",
    "for path in _syspath:\n",
    "    if '/L1BL2Bridge_ATBD/' in _syspath:\n",
    "        break\n",
    "    else:\n",
    "        sys.path.insert(0, os.path.abspath('../../L1BL2Bridge_ATBD/algorithm'))\n",
    "        break\n",
    "from tools import io_handler as io\n",
    "from tools import collocation as coll\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# encoder class for writing JSON files\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    # https://stackoverflow.com/questions/27050108/convert-numpy-type-to-python/27050186#27050186\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, int):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, float):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd32de-1e45-4ad8-8192-05e3cd149c18",
   "metadata": {},
   "source": [
    "## Locate DI-TDS-JNB and configuration\n",
    "\n",
    "The input data for this notebook are organized under [DI-TDS-JNB] (Test Data Set for Jupyter NoteBook). The default is to access this TDS online through thredds / opendap unless the environment variable `CIMR_L2PAD_JNB_TDS` is set. If it is set, it is the path to a local copy of the TDS.\n",
    "\n",
    "We access an L2PP file with all the TBs and auxiliary fields added in the l2pp/L2 Bridge, including OZA compensation terms, as well as atmospheric contribution terms tau, Tup, and Tdown (among others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05582a9b-6e45-4236-83c9-695f42398f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use local TDS: /home/thomasl/Work/L2PAD/local_TDS/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tds_root = os.environ[\"CIMR_L2PAD_JNB_TDS\"]\n",
    "    print(\"Use local TDS: {}\".format(tds_root))\n",
    "except KeyError:\n",
    "    tds_root = 'https://thredds.met.no/thredds/dodsC/cimr/L2PAD/DI-TDS-JNB_draft'\n",
    "    print(\"Use online TDS: {}\".format(tds_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eca7ce",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell has a tag 'parameters' and is used for the CLI with papermill\n",
    "l2pp_archive_dir = os.path.join(tds_root,'CIMR_L2PP')\n",
    "out_dir = '../data/output/L2SIC_dynamic_tuning/'\n",
    "#use_oza_adjust = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7117458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the input parameters\n",
    "if not os.path.isdir(out_dir):\n",
    "    raise ValueError(\"The output directory {} does not exist.\".format(out_dir))\n",
    "if not os.path.isdir(l2pp_archive_dir):\n",
    "    raise ValueError(\"The L2PP archive directory {} does not exist.\".format(l2pp_archive_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995a6da-8943-4057-8d7f-21a8c8fdaa7f",
   "metadata": {},
   "source": [
    "Configure the 3 SIC algorithms we will run as part of the L2 SIC + SIED processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70af9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = dict()\n",
    "algos['KA'] = {'channels':('tb37v', 'tb37h'),  'target_band':'KA'}\n",
    "algos['CKA'] = {'channels':('tb06v', 'tb37v', 'tb37h'), 'target_band':'C'}\n",
    "algos['KKA'] = {'channels':('tb19v', 'tb37v', 'tb37h'), 'target_band':'KU'}\n",
    "\n",
    "tb_dict = {'tb01':'L','tb06':'C','tb10':'X','tb19':'KU','tb37':'KA',}\n",
    "rev_tb_dict = {v:k for k,v in tb_dict.items()}\n",
    "bands_needed = []\n",
    "for alg in algos.keys():\n",
    "    bands_needed += algos[alg]['channels']\n",
    "bands_needed = list(set([tb_dict[b[:-1]] for b in bands_needed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d39be",
   "metadata": {},
   "source": [
    "## An hybrid dynamic / fixed tuning for the SCEPS Polar Scene 1\n",
    "\n",
    "The SCEPS Polar Scene 1 (v2.1, early 2024) does not reach to high-enough SICs, its peak is around 92%-94% and there are very few values above 95%. This means we cannot extract TB samples for near-100% SIC conditions with lat-lon box. On the other hand, we know the sea-ice emissivity model used by SCEPS for this version of the scene is tuned against the CCI RRDP (AMSR-E and AMSR2).\n",
    "\n",
    "We thus go for a hybrid approach (dynamic / fixed) for the tuning of our algorithms:\n",
    "* The OW TB samples are selected from the CIMR l2pp file, in a lat-lon box over open water conditions.\n",
    "* The CI TB samples are taken from the CCI RRDP (AMSR-E and AMSR2) (pan-Arctic domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e704935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OW samples:  15954\n",
      "Number of CI samples:  13441\n"
     ]
    }
   ],
   "source": [
    "# locate and read the RRDP files\n",
    "rrdp_dir = os.path.abspath('../data/CCI_SeaIce_RRDP/')\n",
    "area = 'nh'\n",
    "ow_files, ci_files = rrdp_file.find_rrdp_files(rrdp_dir, area=area, years=(2007, 2013, 2018))\n",
    "\n",
    "channels_needed = []\n",
    "for alg in algos.keys():\n",
    "    channels_needed += algos[alg]['channels']\n",
    "channels_needed = set(channels_needed)\n",
    "\n",
    "rrdp_pos = rrdp_file.read_RRDP_files(area, ow_files, ci_files, channels=channels_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f511800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes of lat/lon: (lat_min, lat_max, lon_min, lon_max)\n",
    "# To simplify, we use lat/lon boxes here, but in an operational\n",
    "# mode the region masks would be defined with more advanced geomasks.\n",
    "OW_box = (73., 76.2, -2., 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958128b",
   "metadata": {},
   "source": [
    "Locate CIMR L2PP files to use in the training. At this stage, we only have 1 realistic swath, so we must use it. But in an operational context the offline training would access a rolling archive of receent L2PP data, typically 5-10 days.\n",
    "\n",
    "To simplify, we only extract OW and CI samples from one swath file from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d449a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_l2pp_fn_patt = 'L2PAD_l2pp_sceps_geo_polar_scene_1_unfiltered_tot_minimal_nom_nedt_apc_tot_v2p1.nc'\n",
    "arch_l2pp_fn_patt = os.path.join(l2pp_archive_dir, arch_l2pp_fn_patt)\n",
    "arch_l2pp_files = glob(arch_l2pp_fn_patt)\n",
    "if len(arch_l2pp_files) == 0:\n",
    "    print(\"WARNING : did not find any good l2pp file in the archive ({})\".format(arch_l2pp_fn_patt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aa8c3",
   "metadata": {},
   "source": [
    "Do the training sample extraction and the storing into tie-point objects for each algorithm in turn (`CKA`, `KKA`, and `KA`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a14102-401f-4026-be13-1c50b5067738",
   "metadata": {},
   "source": [
    "### SIC algorithm tuning against L2PP TBs.\n",
    "\n",
    "We perform 2 types of tuning.\n",
    "1. **OZA-compensated tuning**. We tune the `CKA`, `KKA` and `KA` algorithms on L2PP TBs after OZA-compensation. This is the main tuning, to run the NRT3H and NTC SIC algorithms. During the *online* SIC algorithm, TBs will also be OZA-compensated before entering the algorithms, so we must tune for these TBs here.\n",
    "2. **Per-feed tuning**. The `KKA` algorithm is tuned per-feed, without prior OZA-compensation of the TBs. In the *online* part of the chain, this tuning will be used in the L1B/L2 Bridge to compute the *fastL2* SICs, before OZA-compensation. See the L1B/L2 Bridge ATBD. The other two algorithms are not tuned per-feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02472e3-4361-416e-a462-107c65660887",
   "metadata": {},
   "source": [
    "### OZA-compensated tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa80705e-ed58-45d7-b618-b33b941f719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KA ['KA']\n",
      "OW tie-point for KA ALLFEEDS: [139.44649376 210.87917447]\n",
      "KA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KA_sic_CIMRL2PP-ALLFEEDS.json\n",
      "CKA ['KA', 'C']\n",
      "OW tie-point for CKA ALLFEEDS: [157.31183573 139.4479275  210.87598581]\n",
      "CKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/CKA_sic_CIMRL2PP-ALLFEEDS.json\n",
      "KKA ['KA', 'KU']\n",
      "INFO: Collocate KA -> KU along scan\n",
      "OW tie-point for KKA ALLFEEDS: [182.36086616 139.4382549  210.87589254]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-ALLFEEDS.json\n"
     ]
    }
   ],
   "source": [
    "reload(tp)\n",
    "for algo in algos.keys():\n",
    "    b = algos[algo]['target_band']\n",
    "    bands_needed = list(set([tb_dict[b[:-1]] for b in algos[algo]['channels']]))\n",
    "    print(algo, bands_needed, )\n",
    "    arch_l2pp = io.CIMR_L1B(arch_l2pp_files[0], keep_calibration_view=True, selected_bands=bands_needed)\n",
    "\n",
    "    # the L2PP files are big and contain many variables we do not need for the offline tuning.\n",
    "    #    We drop some variables here.\n",
    "    aux_vars = ('lon', 'lat', 'OZA', 'earth_azimuth',\n",
    "             'scan_angle', 'horn_scan_angle',)\n",
    "    l2pp_vars = ('orig_scan', 'orig_sample', 'orig_horn','brightness_temperature_h', 'brightness_temperature_v',\n",
    "             'oza_compens_RTM_brightness_temperature_v', 'oza_compens_RTM_brightness_temperature_h',)\n",
    "\n",
    "    keep_vars = tuple(list(aux_vars) + list(l2pp_vars))\n",
    "\n",
    "    drop_vars = []\n",
    "    for v in arch_l2pp.data[bands_needed[0]].variables:\n",
    "        if v not in keep_vars:\n",
    "            drop_vars.append(v)\n",
    "\n",
    "    for band in bands_needed:\n",
    "        arch_l2pp.data[band] = arch_l2pp.data[band].drop_vars(drop_vars, )\n",
    "    \n",
    "    # apply the (pre-computed) OZA adjustment fields for all bands.\n",
    "    arch_l2pp.apply_OZA_adjustment()\n",
    "    \n",
    "    # coarsen l2pp samples along the scanlines with a kernel of 5 (horns are *not* combined)\n",
    "    arch_l2pp = arch_l2pp.coarsen_along_scanlines(kernel=5)\n",
    "    \n",
    "    # collocate TBs to the target band of the algorithm.\n",
    "    if algo == 'KA':\n",
    "        # we use only one frequency: no need for collocation\n",
    "        pass\n",
    "    elif algo == 'KKA':\n",
    "        # the collocation is done along scanlines (to not mix feeds with different OZAs)\n",
    "        arch_l2pp = arch_l2pp.collocate_along_scan(target_band=b)\n",
    "    elif algo == 'CKA':\n",
    "        # the collocation is done across scanlines and mixes feeds with different OZAs\n",
    "        #    note : here we also mix forward and aft- scans which is maybe not optimal.\n",
    "        arch_l2pp = coll.collocate_channels(arch_l2pp.data, b, method='nn')\n",
    "    else:\n",
    "        raise NotImplementedError(\"Collocation step missing for {}\".format(algo))\n",
    "    \n",
    "    # how to access the lat and lon depends on the collocation type\n",
    "    if isinstance(arch_l2pp, io.CIMR_L1B):\n",
    "        _lat = arch_l2pp.data[b].lat\n",
    "        _lon = arch_l2pp.data[b].lon\n",
    "    elif str(arch_l2pp['_type']) == 'L1C swath':\n",
    "        _lat = arch_l2pp['geolocation']['lat']\n",
    "        _lon = arch_l2pp['geolocation']['lon']\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized L2PP data type.\")\n",
    "    \n",
    "    # Create the masks for OW samples from the lat-lon box\n",
    "    _mask = dict()\n",
    "    _mask['ow'] = (_lat > OW_box[0])*(_lat < OW_box[1])*(_lon > OW_box[2])*(_lon < OW_box[3]).astype('int')\n",
    "    \n",
    "    # Prepare for extracting the training samples. \n",
    "    dyn_tbs = dict()\n",
    "    dyn_tbs['ow'] = dict()\n",
    "    \n",
    "    # Extract the brightness temperatures in the OW areas and store in sample dictionaries\n",
    "    for ch in algos[algo]['channels']:\n",
    "        # for each input channel to the algorithm (e.g. tb19v), deduce the\n",
    "        #   name of the variable to be read in the l2pp (or L1C) data structure.\n",
    "        bn = tb_dict[ch[:-1]]\n",
    "        pol = ch[-1:]\n",
    "        \n",
    "        if isinstance(arch_l2pp, io.CIMR_L1B):\n",
    "            bnstr = ''\n",
    "            if bn != b:\n",
    "                bnstr = '{}_'.format(bn)\n",
    "            tb_n = '{}brightness_temperature_{}'.format(bnstr, pol)\n",
    "            # apply the OW selection mask\n",
    "            w = 'ow'\n",
    "            dyn_tbs[w][ch] = arch_l2pp.data[b][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "        elif str(arch_l2pp['_type']) == 'L1C swath':\n",
    "            tb_n = 'brightness_temperature_{}'.format(pol)\n",
    "            # apply the OW selection mask\n",
    "            w = 'ow'\n",
    "            dyn_tbs[w][ch] = arch_l2pp[bn+'_BAND'][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized L2PP data type.\")\n",
    "            \n",
    "    #\n",
    "    # OZA-COMPENSATED TUNING, COMBINING ALL FEEDS INTO ONE ALGORITHM\n",
    "    #\n",
    "    \n",
    "    # Transfer the training data in a tie-point object. This involves some processing,\n",
    "    #  like computing the tie-points and their covariance matrices. \n",
    "    ow_tp = tp.OWTiepoint(source='CIMRl2pp-ALLFEEDS', tbs=dyn_tbs['ow'])\n",
    "    ci_tp = tp.CICETiepoint(source='CCI-RRDP', tbs=rrdp_pos['ci'])\n",
    "    ow_tp.instr = ci_tp.instr = 'CIMR'\n",
    "    print(\"OW tie-point for {} ALLFEEDS: {}\".format(algo, ow_tp.tp))\n",
    "\n",
    "    # Tune the SIC algorithm on these tie-points\n",
    "    tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "\n",
    "    # Store the algorithm on disk (JSON file) for later re-use\n",
    "    json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "    with open(json_fn, 'w') as fp_out:\n",
    "        json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "    print(\"{} SIC configuration file is in {}\".format(algo, json_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9846525-17e5-4b4b-a029-0fd8287018ba",
   "metadata": {},
   "source": [
    "### Per-feed tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f350a03b-b729-4ade-8222-3466582ce38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKA ['KA', 'KU']\n",
      "INFO: Collocate KA -> KU along scan\n",
      "OW tie-point for KKA FEED0: [184.08696665 139.26120481 212.3442934 ]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED0.json\n",
      "OW tie-point for KKA FEED1: [183.56648083 139.32548574 211.90311752]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED1.json\n",
      "OW tie-point for KKA FEED2: [183.0547305  139.37276532 211.46769661]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED2.json\n",
      "OW tie-point for KKA FEED3: [182.54003186 139.40239228 211.03559911]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED3.json\n",
      "OW tie-point for KKA FEED4: [182.04272814 139.48167391 210.60163844]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED4.json\n",
      "OW tie-point for KKA FEED5: [181.47872952 139.5071235  210.14570276]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED5.json\n",
      "OW tie-point for KKA FEED6: [180.98259006 139.57072539 209.70125213]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED6.json\n",
      "OW tie-point for KKA FEED7: [180.5425952  139.63521035 209.28653768]\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CIMRL2PP-FEED7.json\n"
     ]
    }
   ],
   "source": [
    "for algo in ('KKA',):\n",
    "    \n",
    "    b = algos[algo]['target_band']\n",
    "    bands_needed = list(set([tb_dict[b[:-1]] for b in algos[algo]['channels']]))\n",
    "    print(algo, bands_needed, )\n",
    "\n",
    "    arch_l2pp = io.CIMR_L1B(arch_l2pp_files[0], keep_calibration_view=True, selected_bands=bands_needed)\n",
    "\n",
    "    # the L2PP files are big and contain many variables we do not need for the offline tuning.\n",
    "    #    We drop some variables here.\n",
    "    aux_vars = ('lon', 'lat', 'OZA', 'earth_azimuth',\n",
    "             'scan_angle', 'horn_scan_angle',)\n",
    "    l2pp_vars = ('orig_scan', 'orig_sample', 'orig_horn','brightness_temperature_h', 'brightness_temperature_v',)\n",
    "\n",
    "    keep_vars = tuple(list(aux_vars) + list(l2pp_vars))\n",
    "\n",
    "    drop_vars = []\n",
    "    for v in arch_l2pp.data[bands_needed[0]].variables:\n",
    "        if v not in keep_vars:\n",
    "            drop_vars.append(v)\n",
    "\n",
    "    for band in bands_needed:\n",
    "        arch_l2pp.data[band] = arch_l2pp.data[band].drop_vars(drop_vars, )\n",
    "    \n",
    "    # coarsen l2pp samples along the scanlines with a kernel of 5 (horns are *not* combined)\n",
    "    arch_l2pp = arch_l2pp.coarsen_along_scanlines(kernel=5)\n",
    "    \n",
    "    # collocate TBs to the target band of the algorithm.\n",
    "    if algo == 'KA':\n",
    "        # we use only one frequency: no need for collocation\n",
    "        pass\n",
    "    elif algo == 'KKA':\n",
    "        # the collocation is done along scanlines (to not mix feeds with different OZAs)\n",
    "        arch_l2pp = arch_l2pp.collocate_along_scan(target_band=b)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Per-feed tuning not implemented (not possible?) for {}\".format(algo))\n",
    "    \n",
    "    # how to access the lat and lon depends on the collocation type\n",
    "    if isinstance(arch_l2pp, io.CIMR_L1B):\n",
    "        _lat = arch_l2pp.data[b].lat\n",
    "        _lon = arch_l2pp.data[b].lon\n",
    "    elif str(arch_l2pp['_type']) == 'L1C swath':\n",
    "        _lat = arch_l2pp['geolocation']['lat']\n",
    "        _lon = arch_l2pp['geolocation']['lon']\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized L2PP data type.\")\n",
    "    \n",
    "    # Create the masks for OW samples from the lat-lon box\n",
    "    _mask = dict()\n",
    "    _mask['ow'] = (_lat > OW_box[0])*(_lat < OW_box[1])*(_lon > OW_box[2])*(_lon < OW_box[3]).astype('int')\n",
    "    \n",
    "    # Prepare for extracting the training samples. We also store the feed number (e.g. 0-7 for KU/KA)\n",
    "    #   to be able to later train algorithm for specific feeds.\n",
    "    dyn_tbs = dict()\n",
    "    dyn_tbs['ow'] = dict()\n",
    "    dyn_tbs_feed = dict()\n",
    "    dyn_tbs_feed['ow'] = None\n",
    "    \n",
    "    # Extract the brightness temperatures in the OW areas and store in sample dictionaries\n",
    "    for ch in algos[algo]['channels']:\n",
    "        # for each input channel to the algorithm (e.g. tb19v), deduce the\n",
    "        #   name of the variable to be read in the l2pp (or L1C) data structure.\n",
    "        bn = tb_dict[ch[:-1]]\n",
    "        pol = ch[-1:]\n",
    "        \n",
    "        if isinstance(arch_l2pp, io.CIMR_L1B):\n",
    "            bnstr = ''\n",
    "            if bn != b:\n",
    "                bnstr = '{}_'.format(bn)\n",
    "            tb_n = '{}brightness_temperature_{}'.format(bnstr, pol)\n",
    "            # apply the OW selection mask\n",
    "            w = 'ow'\n",
    "            dyn_tbs[w][ch] = arch_l2pp.data[b][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "            # keep the feed number information only for the target band\n",
    "            if bn == b and dyn_tbs_feed[w] is None:\n",
    "                dyn_tbs_feed[w] = arch_l2pp.data[b]['orig_horn'].where(_mask[w]).to_masked_array().compressed()\n",
    "        elif str(arch_l2pp['_type']) == 'L1C swath':\n",
    "            tb_n = 'brightness_temperature_{}'.format(pol)\n",
    "            # apply the OW selection mask\n",
    "            w = 'ow'\n",
    "            dyn_tbs[w][ch] = arch_l2pp[bn+'_BAND'][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "            # keep the feed number information only for the target band\n",
    "            if bn == b and dyn_tbs_feed[w] is None:\n",
    "                dyn_tbs_feed[w] = arch_l2pp[bn+'_BAND']['orig_horn'].where(_mask[w]).to_masked_array().compressed()\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized L2PP data type.\")\n",
    "    \n",
    "    #\n",
    "    # PER-FEED TUNING, ONE FOR EACH FEED, no OZA compensation\n",
    "    #\n",
    "    for feed in range(0, io.n_horns[b]):\n",
    "        FEEDNB = 'FEED{}'.format(feed)\n",
    "        \n",
    "        dyn_tbs_f = dict()\n",
    "        dyn_tbs_f['ow'] = dict()\n",
    "        \n",
    "        for ch in algos[algo]['channels']:\n",
    "            for w in ('ow',):\n",
    "                dyn_tbs_f[w][ch] = dyn_tbs[w][ch][dyn_tbs_feed[w]==feed]\n",
    "        \n",
    "        ow_tp = tp.OWTiepoint(source='CIMRl2pp-{}'.format(FEEDNB), tbs=dyn_tbs_f['ow'])\n",
    "        ci_tp = tp.CICETiepoint(source='CCI-RRDP', tbs=rrdp_pos['ci'])\n",
    "        ow_tp.instr = ci_tp.instr = 'CIMR'\n",
    "        print(\"OW tie-point for {} {}: {}\".format(algo, FEEDNB, ow_tp.tp))\n",
    "        \n",
    "        # Tune the SIC algorithm on these tie-points\n",
    "        tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "\n",
    "        # Store the algorithm on disk (JSON file) for later re-use\n",
    "        json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "        with open(json_fn, 'w') as fp_out:\n",
    "            json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "        print(\"{} SIC configuration file is in {}\".format(algo, json_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa0cc6",
   "metadata": {},
   "source": [
    "## SIC tuning against static dataset (ESA CCI RRDP)\n",
    "\n",
    "At the end, we also run a tuning of each algorithm against the static set of SIC0 and SIC1 data points\n",
    "from the ESA CCI Round Robin Data Package (AMSR-E + AMSR2 TBs). This would not be used in the operational processing, but is used now to demonstrate the impact of dynamic tuning of the algorithms in the Performance Assessment chapter.\n",
    "\n",
    "The training data are taken from the ESA CCI Sea Ice Concentration Round Robin Data Package of Pedersen et al. (2019). The relevant data files as well as routines to parse the files are stored in module siddrp/.\n",
    "\n",
    "> Pedersen, Leif Toudal; Saldo, Roberto; Ivanova, Natalia; Kern, Stefan; Heygster, Georg; Tonboe, Rasmus; et al. (2019): Reference dataset for sea ice concentration. figshare. Dataset. https://doi.org/10.6084/m9.figshare.6626549.v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220fcd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KA_sic_CCI-RRDP.json\n",
      "CKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/CKA_sic_CCI-RRDP.json\n",
      "KKA SIC configuration file is in ../data/output/L2SIC_dynamic_tuning/KKA_sic_CCI-RRDP.json\n"
     ]
    }
   ],
   "source": [
    "# run the tuning for each algorithm in turn\n",
    "for algo in algos.keys():\n",
    "    \n",
    "    # Transfer the training data in a tie-point object. This involves some processing,\n",
    "    #  like computing the tie-points and their covariance matrices. \n",
    "    ow_tp = tp.OWTiepoint(source='CCI-RRDP', tbs=rrdp_pos['ow'])\n",
    "    ci_tp = tp.CICETiepoint(source='CCI-RRDP', tbs=rrdp_pos['ci'])\n",
    "    ow_tp.instr = ci_tp.instr = 'AMSRs'\n",
    "    ow_tp.source = ci_tp.source = 'CCI-RRDP'\n",
    "    \n",
    "    # Tune the SIC algorithm on these tie-points\n",
    "    tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "    \n",
    "    # Store the algorithm on disk (JSON file) for later re-use\n",
    "    json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "    with open(json_fn, 'w') as fp_out:\n",
    "        json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "    print(\"{} SIC configuration file is in {}\".format(algo, json_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e4536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
